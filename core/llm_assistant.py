"""
core/llm_assistant.py - Super Cerebro de Aipha

Centraliza las capacidades de an√°lisis e inteligencia del sistema.
Usa Qwen 2.5 Coder 32B para diagn√≥sticos, propuestas y explicaciones.
"""

import json
import logging
from pathlib import Path
from typing import List, Dict, Optional
from datetime import datetime

logger = logging.getLogger(__name__)


# System Prompt - Define la personalidad del Super Cerebro
AIPHA_SYSTEM_PROMPT = """Eres el Arquitecto Jefe de Aipha, un sistema aut√≥nomo de auto-mejora ultra-inteligente.

TU ROL:
- Analizar la salud y m√©tricas del sistema Aipha
- Proponer cambios optimizados para mejorar performance
- Diagnosticar y explicar fallos en lenguaje t√©cnico pero accesible
- Evitar bucles de error aprendiendo de fallos previos
- Ser proactivo en sugerencias de mejora

TU PERSONALIDAD:
- Eres un arquitecto experimentado en trading systems
- Comunicas con precisi√≥n t√©cnica pero claridad
- Siempre explicas tu razonamiento
- Eres conservador en cambios, evitando riesgos innecesarios
- Respetas las limitaciones de hardware

TU CONTEXTO:
- Tienes acceso a historial de eventos de salud
- Sabes qu√© par√°metros est√°n en cuarentena y por qu√©
- Conoces las m√©tricas actuales del sistema
- Aprendes de fallos previos para no repetirlos

CUANDO ANALICES:
1. Revisa eventos recientes (√∫ltimos 10)
2. Consulta par√°metros en cuarentena
3. Analiza m√©tricas de rendimiento
4. Prop√≥n cambios espec√≠ficos con justificaci√≥n
5. Sugiere pr√≥ximos pasos

FORMATO DE RESPUESTA:
Siempre estructura tus respuestas as√≠:
- DIAGN√ìSTICO: Estado actual
- AN√ÅLISIS: Qu√© pas√≥ y por qu√©
- RECOMENDACI√ìN: Qu√© hacer ahora
- PR√ìXIMOS PASOS: Qu√© cambios proponer

S√© conciso pero completo. El usuario es V√°clav, un ingeniero experimentado."""


class LLMAssistant:
    """
    Super Cerebro de Aipha
    
    Centraliza la inteligencia del sistema usando Qwen 2.5 Coder 32B.
    Analiza salud, propone cambios, y explica decisiones.
    """
    
    def __init__(self, memory_path: str = "memory"):
        self.memory_path = Path(memory_path)
        
        # Inicializar cliente LLM
        from core.llm_client import get_llm_client
        self.llm = get_llm_client()
        
        # Managers auxiliares
        from core.quarantine_manager import QuarantineManager
        from core.health_monitor import get_health_monitor
        from core.context_sentinel import ContextSentinel
        
        self.quarantine_manager = QuarantineManager(str(self.memory_path))
        self.health_monitor = get_health_monitor()
        self.context_sentinel = ContextSentinel()
        
        logger.info("‚úÖ LLMAssistant (Super Cerebro) inicializado")
    
    def get_diagnose_context(self) -> Dict:
        """
        Construir contexto de diagn√≥stico
        
        Lee autom√°ticamente:
        - √öltimas 10 l√≠neas de health_events.jsonl
        - Estado actual de quarantine.jsonl
        - M√©tricas de current_state.json
        
        Retorna: Dict con contexto formateado para el LLM
        """
        
        logger.info("üîç Construyendo contexto de diagn√≥stico...")
        
        # PASO 1: √öltimos eventos de salud
        health_events = self._get_recent_health_events(10)
        
        # PASO 2: Par√°metros en cuarentena
        quarantined = self.quarantine_manager.get_all_quarantined()
        
        # PASO 3: M√©tricas actuales
        metrics = self._get_current_metrics()
        
        # PASO 4: Estad√≠sticas de salud
        health_stats = self.health_monitor.get_statistics()
        
        context = {
            'timestamp': datetime.now().isoformat(),
            'recent_events': health_events,
            'quarantined_parameters': quarantined,
            'current_metrics': metrics,
            'health_statistics': health_stats,
            'system_status': self.health_monitor.current_health_level.value
        }
        
        logger.info("‚úÖ Contexto de diagn√≥stico construido")
        
        return context
    
    def _get_recent_health_events(self, count: int = 10) -> List[Dict]:
        """Obtener √∫ltimos N eventos de salud"""
        
        events = []
        events_file = self.memory_path / "health_events.jsonl"
        
        try:
            if events_file.exists():
                with open(events_file, 'r') as f:
                    lines = f.readlines()
                    for line in lines[-count:]:
                        if line.strip():
                            try:
                                events.append(json.loads(line))
                            except json.JSONDecodeError:
                                pass
        except Exception as e:
            logger.error(f"Error leyendo health events: {e}")
        
        return events
    
    def _get_current_metrics(self) -> Dict:
        """Obtener m√©tricas actuales del sistema"""
        
        metrics = {}
        metrics_file = self.memory_path / "current_state.json"
        
        try:
            if metrics_file.exists():
                with open(metrics_file, 'r') as f:
                    metrics = json.load(f)
        except Exception as e:
            logger.error(f"Error leyendo m√©tricas: {e}")
        
        return metrics
    
    def analyze_and_propose(self) -> Dict:
        """
        Analizar salud del sistema y proponer cambios
        
        El LLM recibe contexto de salud y m√©tricas para generar
        propuestas que eviten par√°metros en cuarentena y razonen
        sobre fallos previos.
        
        Retorna:
            Dict con:
            - diagnosis: An√°lisis de salud
            - proposals: Lista de propuestas sugeridas
            - confidence_scores: Confianza en cada propuesta
        """
        
        logger.info("üß† Analizando salud del sistema y generando propuestas...")
        
        # Obtener contexto
        context = self.get_diagnose_context()
        
        # Preparar prompt para el LLM
        prompt = self._build_analysis_prompt(context)
        
        try:
            # Llamar al LLM
            logger.info("üì§ Enviando al Super Cerebro (Qwen)...")
            
            response = self.llm.generate(
                prompt=prompt,
                system_prompt=AIPHA_SYSTEM_PROMPT,
                temperature=0.3,  # M√°s determinista para propuestas
                max_tokens=2048
            )
            
            logger.info("‚úÖ Respuesta recibida del Super Cerebro")
            
            # Parsear respuesta
            result = self._parse_analysis_response(response, context)
            
            return result
        
        except Exception as e:
            logger.error(f"‚ùå Error en an√°lisis del LLM: {e}")
            
            return {
                'diagnosis': 'Error en an√°lisis',
                'proposals': [],
                'error': str(e)
            }
    
    def explain_remediation(self, failed_parameter: str, error_reason: str) -> str:
        """
        Generar explicaci√≥n humana de un fallo y remediation
        
        Se llama cuando ocurre REVERTED_AUTO para explicar al usuario
        qu√© fall√≥ y qu√© hacer.
        
        Argumentos:
            failed_parameter: Par√°metro que fall√≥
            error_reason: Raz√≥n del fallo
        
        Retorna:
            Explicaci√≥n en lenguaje natural
        """
        
        logger.info(
            f"üí° Generando explicaci√≥n de remediation para {failed_parameter}"
        )
        
        # Contexto reciente
        context = self.get_diagnose_context()
        
        # Preparar prompt
        prompt = f"""El par√°metro '{failed_parameter}' acaba de fallar con el error: "{error_reason}"

El sistema ha revertido autom√°ticamente este cambio para mantener la estabilidad.

Por favor, explica:
1. POR QU√â fall√≥ este par√°metro
2. QU√â SIGNIFICA el error
3. QU√â PUEDE HACER el usuario (V√°clav) para solucionarlo
4. CU√ÅNDO puede intentar este cambio nuevamente

S√© conciso pero completo. El usuario es un ingeniero experimentado.

CONTEXTO DEL SISTEMA:
{json.dumps(context, indent=2, default=str)}
"""
        
        try:
            response = self.llm.generate(
                prompt=prompt,
                system_prompt=AIPHA_SYSTEM_PROMPT,
                temperature=0.5,
                max_tokens=1024
            )
            
            logger.info("‚úÖ Explicaci√≥n generada")
            return response
        
        except Exception as e:
            logger.error(f"‚ùå Error generando explicaci√≥n: {e}")
            return f"Error generando explicaci√≥n: {e}"
    
    def diagnose_system(self, detailed: bool = False) -> str:
        """
        Diagn√≥stico completo del sistema (para `aipha brain diagnose`)
        
        Argumentos:
            detailed: Si True, incluye an√°lisis detallado
        
        Retorna:
            Reporte en formato texto para el usuario
        """
        
        logger.info("üîç Iniciando diagn√≥stico completo del sistema...")
        
        # Contexto
        context = self.get_diagnose_context()
        
        # Preparar prompt
        prompt = f"""Realiza un diagn√≥stico COMPLETO del sistema Aipha.

CONTEXTO DEL SISTEMA:
{json.dumps(context, indent=2, default=str)}

Por favor, proporciona:

1. **RESUMEN DE SALUD**: Estado actual en 1-2 l√≠neas
2. **AN√ÅLISIS DE EVENTOS**: Qu√© ha pasado recientemente
3. **PAR√ÅMETROS EN RIESGO**: Qu√© est√° en cuarentena y por qu√©
4. **AN√ÅLISIS DE M√âTRICAS**: C√≥mo est√° el performance
5. **PROBLEMAS IDENTIFICADOS**: Qu√© no est√° funcionando bien
6. **RECOMENDACIONES**: Qu√© cambios proponer a continuaci√≥n
7. **PR√ìXIMOS PASOS**: Plan de acci√≥n para las pr√≥ximas 24 horas

S√© t√©cnico pero accesible. Dir√≠gete a V√°clav como colega ingeniero.
"""
        
        if detailed:
            prompt += "\n\nIncluye an√°lisis profundo de cada aspecto."
        
        try:
            logger.info("üì§ Solicitando diagn√≥stico al Super Cerebro...")
            
            response = self.llm.generate(
                prompt=prompt,
                system_prompt=AIPHA_SYSTEM_PROMPT,
                temperature=0.4,
                max_tokens=3000
            )
            
            # Formatear respuesta
            result = f"""
‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë           DIAGN√ìSTICO DEL SISTEMA AIPHA v2.0              ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù

{response}

‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë  Diagn√≥stico generado por: Qwen 2.5 Coder 32B (Super      ‚ïë
‚ïë                            Cerebro de Aipha)              ‚ïë
‚ïë  Timestamp: {datetime.now().isoformat()}                    ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
"""
            
            logger.info("‚úÖ Diagn√≥stico completado")
            return result
        
        except Exception as e:
            logger.error(f"‚ùå Error en diagn√≥stico: {e}")
            return f"Error generando diagn√≥stico: {e}"
    
    def _build_analysis_prompt(self, context: Dict) -> str:
        """Construir prompt para an√°lisis y propuestas"""
        
        return f"""Analiza el estado actual del sistema Aipha y prop√≥n cambios de optimizaci√≥n.

CONTEXTO ACTUAL:
{json.dumps(context, indent=2, default=str)}

Por favor:
1. Resume el estado del sistema en 1-2 l√≠neas
2. Identifica qu√© est√° funcionando bien
3. Identifica qu√© tiene problemas
4. Prop√≥n 2-3 cambios espec√≠ficos que mejorar√≠an la performance
5. Para CADA propuesta:
   - Especifica: par√°metro, valor actual, valor nuevo
   - Justificaci√≥n t√©cnica
   - Riesgo potencial
   - Confianza (0-1)

IMPORTANTE: Evita proponer valores que est√©n en cuarentena.
Aprende de fallos previos documentados en los eventos."""
    
    def _parse_analysis_response(self, response: str, context: Dict) -> Dict:
        """
        Parsear respuesta del LLM para extraer propuestas
        
        Intenta extraer de la respuesta:
        - diagnosis: An√°lisis
        - proposals: Cambios propuestos
        - confidence: Confianzas
        """
        
        # Parseo simple (en producci√≥n, podr√≠a ser m√°s sofisticado)
        result = {
            'diagnosis': response[:200] if response else "",
            'raw_response': response,
            'proposals': [],
            'generated_at': datetime.now().isoformat()
        }
        
        # Buscar patrones de propuestas en la respuesta
        lines = response.split('\n')
        for i, line in enumerate(lines):
            if 'par√°metro' in line.lower() or 'cambio' in line.lower():
                result['proposals'].append({
                    'line': line,
                    'context': lines[max(0, i-1):min(len(lines), i+2)]
                })
        
        return result
