#!/usr/bin/env python3
"""
test_diagnostic_enhancements.py

Test de las mejoras en el sistema de diagnÃ³stico:
1. DetecciÃ³n de intervenciones manuales (USER vs AUTO)
2. AnÃ¡lisis de impacto en mÃ©tricas
3. Capacidad del LLM de entender reasoning del usuario
"""

import json
import sys
from pathlib import Path

# Setup paths
sys.path.insert(0, str(Path(__file__).parent))

from core.llm_assistant import LLMAssistant


def test_get_diagnose_context():
    """Verificar que get_diagnose_context() retorna contexto enriquecido"""
    
    print("\n" + "=" * 70)
    print("TEST 1: get_diagnose_context() retorna contexto enriquecido")
    print("=" * 70)
    
    assistant = LLMAssistant(memory_path="memory")
    context = assistant.get_diagnose_context()
    
    # Verificaciones
    assert 'simulation_mode' in context, "âŒ Falta: simulation_mode"
    assert 'manual_interventions_detail' in context, "âŒ Falta: manual_interventions_detail"
    assert 'user_actions' in context, "âŒ Falta: user_actions"
    assert 'auto_actions' in context, "âŒ Falta: auto_actions"
    assert 'action_history' in context, "âŒ Falta: action_history"
    assert 'impact_analysis' in context, "âŒ Falta: impact_analysis"
    assert 'system_context' in context, "âŒ Falta: system_context"
    
    print("âœ… Todos los campos requeridos presentes")
    
    # Mostrar datos
    print(f"\nğŸ“Š AnÃ¡lisis de Contexto:")
    print(f"  â€¢ Modo SimulaciÃ³n: {context.get('simulation_mode')}")
    print(f"  â€¢ Intervenciones Manuales: {context.get('manual_interventions')}")
    print(f"  â€¢ Acciones del Usuario: {len(context.get('user_actions', []))}")
    print(f"  â€¢ Acciones AutomÃ¡ticas: {len(context.get('auto_actions', []))}")
    print(f"  â€¢ Total de Acciones: {len(context.get('action_history', []))}")
    
    # Mostrar intervenciones manuales
    if context.get('manual_interventions_detail'):
        print(f"\nğŸ¯ Intervenciones Manuales Detectadas:")
        for detail in context.get('manual_interventions_detail', []):
            print(f"  â€¢ {detail.get('component')}.{detail.get('parameter')} = {detail.get('new_value')}")
            print(f"    - RazÃ³n: {detail.get('reason')}")
            print(f"    - Score: {detail.get('score')}")
            print(f"    - Creado por: {detail.get('created_by')}")
    
    # Mostrar impacto
    impact = context.get('impact_analysis', {})
    print(f"\nğŸ’¥ AnÃ¡lisis de Impacto:")
    print(f"  â€¢ Total intervenciones: {impact.get('total_interventions', 0)}")
    print(f"  â€¢ Win Rate actual: {impact.get('win_rate_current', 0)*100:.1f}%")
    print(f"  â€¢ Drawdown actual: {impact.get('drawdown_current', 0)*100:.1f}%")
    print(f"  â€¢ Resumen: {impact.get('impact_summary', 'N/A')}")
    
    return True


def test_classify_actions():
    """Verificar que se clasifican correctamente acciones USER vs AUTO"""
    
    print("\n" + "=" * 70)
    print("TEST 2: ClasificaciÃ³n de acciones USER vs AUTO")
    print("=" * 70)
    
    assistant = LLMAssistant(memory_path="memory")
    context = assistant.get_diagnose_context()
    
    user_actions = context.get('user_actions', [])
    auto_actions = context.get('auto_actions', [])
    
    print(f"\nğŸ“‹ Acciones del Usuario (CLI):")
    if user_actions:
        for action in user_actions:
            print(f"  â€¢ {action.get('timestamp', 'N/A')[:19]}: {action.get('component')}")
    else:
        print("  (Sin acciones del usuario)")
    
    print(f"\nğŸ¤– Acciones AutomÃ¡ticas (Sistema):")
    if auto_actions:
        for i, action in enumerate(auto_actions[:5], 1):
            print(f"  {i}. {action.get('timestamp', 'N/A')[:19]}: {action.get('agent')} en {action.get('component')}")
        if len(auto_actions) > 5:
            print(f"  ... y {len(auto_actions) - 5} mÃ¡s")
    else:
        print("  (Sin acciones automÃ¡ticas)")
    
    return True


def test_diagnose_system_simple():
    """Verificar diagnÃ³stico simple sin LLM"""
    
    print("\n" + "=" * 70)
    print("TEST 3: diagnose_system() - Modo Simple (sin LLM)")
    print("=" * 70)
    
    assistant = LLMAssistant(memory_path="memory")
    result = assistant.diagnose_system(detailed=False)
    
    # Verificaciones
    assert 'diagnosis' in result, "âŒ Falta: diagnosis"
    assert 'manual_interventions_detail' in result, "âŒ Falta: manual_interventions_detail"
    assert 'simulation_mode' in result, "âŒ Falta: simulation_mode"
    assert 'impact_analysis' in result, "âŒ Falta: impact_analysis"
    assert 'llm_analysis' not in result, "âŒ llm_analysis no deberÃ­a estar en modo simple"
    
    print("âœ… Estructura de resultado correcta")
    
    print(f"\nğŸ“Š DiagnÃ³stico Simple:")
    print(f"  â€¢ Intervenciones Manuales: {result.get('manual_interventions', 0)}")
    print(f"  â€¢ Modo SimulaciÃ³n: {result.get('simulation_mode', False)}")
    
    # Mostrar extracto del diagnÃ³stico
    diagnosis = result.get('diagnosis', '')
    if diagnosis:
        lines = diagnosis.split('\n')[:10]
        print("\nğŸ“ Primeras lÃ­neas del diagnÃ³stico:")
        for line in lines:
            if line.strip():
                print(f"  {line}")
    
    return True


def test_system_context_format():
    """Verificar que system_context estÃ¡ formateado correctamente para el LLM"""
    
    print("\n" + "=" * 70)
    print("TEST 4: system_context - Formato para el LLM")
    print("=" * 70)
    
    assistant = LLMAssistant(memory_path="memory")
    context = assistant.get_diagnose_context()
    
    system_context = context.get('system_context', '')
    
    # Verificaciones
    assert 'CONTEXTO DEL SISTEMA' in system_context, "âŒ Falta header"
    assert 'Estado General' in system_context, "âŒ Falta secciÃ³n Estado"
    assert 'Intervenciones Manuales' in system_context, "âŒ Falta secciÃ³n Intervenciones"
    assert 'Cambios AutomÃ¡ticos' in system_context, "âŒ Falta secciÃ³n Cambios"
    
    print("âœ… Estructura de system_context correcta")
    
    print("\nğŸ“ System Context (primeras 800 caracteres):")
    print(system_context[:800])
    print("\n... (truncado)")
    
    return True


def test_impact_analysis():
    """Verificar que el anÃ¡lisis de impacto es correcto"""
    
    print("\n" + "=" * 70)
    print("TEST 5: Impact Analysis - CorrelaciÃ³n intervenciones/mÃ©tricas")
    print("=" * 70)
    
    assistant = LLMAssistant(memory_path="memory")
    context = assistant.get_diagnose_context()
    
    impact = context.get('impact_analysis', {})
    
    print(f"\nğŸ“Š Datos de Impacto:")
    print(f"  â€¢ Total intervenciones aplicadas: {impact.get('total_interventions', 0)}")
    print(f"  â€¢ Win Rate actual: {impact.get('win_rate_current', 0)*100:.1f}%")
    print(f"  â€¢ Drawdown actual: {impact.get('drawdown_current', 0)*100:.1f}%")
    
    if impact.get('latest_intervention'):
        latest = impact.get('latest_intervention')
        print(f"\nğŸ¯ Ãšltima IntervenciÃ³n:")
        print(f"  â€¢ Componente: {latest.get('component')}")
        print(f"  â€¢ ParÃ¡metro: {latest.get('parameter')}")
        print(f"  â€¢ Nuevo Valor: {latest.get('new_value')}")
        print(f"  â€¢ RazÃ³n: {latest.get('reason')}")
        print(f"  â€¢ Timestamp: {latest.get('timestamp')}")
    
    print(f"\nğŸ’¡ Resumen de Impacto:")
    print(f"  {impact.get('impact_summary', 'N/A')}")
    
    return True


def run_all_tests():
    """Ejecutar todos los tests"""
    
    print("\n" + "ğŸ§ª " * 25)
    print("TEST SUITE: Mejoras en Sistema de DiagnÃ³stico")
    print("ğŸ§ª " * 25)
    
    tests = [
        ("get_diagnose_context()", test_get_diagnose_context),
        ("classify_actions()", test_classify_actions),
        ("diagnose_system() simple", test_diagnose_system_simple),
        ("system_context format", test_system_context_format),
        ("impact_analysis", test_impact_analysis),
    ]
    
    passed = 0
    failed = 0
    
    for test_name, test_func in tests:
        try:
            if test_func():
                passed += 1
        except Exception as e:
            print(f"\nâŒ TEST FALLIDO: {test_name}")
            print(f"   Error: {e}")
            import traceback
            traceback.print_exc()
            failed += 1
    
    # Resumen
    print("\n" + "=" * 70)
    print("RESUMEN DE TESTS")
    print("=" * 70)
    print(f"âœ… Pasaron: {passed}/{len(tests)}")
    print(f"âŒ Fallaron: {failed}/{len(tests)}")
    
    if failed == 0:
        print("\nğŸ‰ Â¡TODOS LOS TESTS PASARON!")
        return 0
    else:
        print("\nâš ï¸  Algunos tests fallaron")
        return 1


if __name__ == "__main__":
    sys.exit(run_all_tests())
